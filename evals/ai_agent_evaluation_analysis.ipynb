{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa2779d",
   "metadata": {},
   "source": [
    "# AI Agent Evaluation Analysis Dashboard\n",
    "\n",
    "This notebook provides analysis tools for the monitoring and evaluation framework implemented for the AI agent project. It helps you visualize metrics, analyze performance, and compare different agent configurations.\n",
    "\n",
    "## Features\n",
    "- Load and process evaluation results from the evaluation framework\n",
    "- Visualize key metrics for agent performance\n",
    "- Analyze user feedback and satisfaction\n",
    "- Compare different agent configurations (A/B testing)\n",
    "- Generate insights and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cba8b",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries\n",
    "\n",
    "First, let's install the necessary packages and import libraries for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install matplotlib seaborn pandas numpy scikit-learn plotly tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ba3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Ensure plots are displayed inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedb504",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Data\n",
    "\n",
    "Let's load the evaluation results from the `evaluation_results` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b11112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "EVAL_RESULTS_PATH = Path('../evals/evaluation_results')\n",
    "FEEDBACK_PATH = Path('../evals/feedback_data.json')\n",
    "\n",
    "def load_latest_evaluation():\n",
    "    \"\"\"Load the latest evaluation result file\"\"\"\n",
    "    try:\n",
    "        # First check if there's a latest.json file\n",
    "        latest_path = EVAL_RESULTS_PATH / 'latest.json'\n",
    "        if latest_path.exists():\n",
    "            with open(latest_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        # If not, find the most recent file\n",
    "        files = list(EVAL_RESULTS_PATH.glob('*.json'))\n",
    "        if not files:\n",
    "            return None\n",
    "\n",
    "        # Sort by modification time (most recent first)\n",
    "        latest_file = max(files, key=lambda f: f.stat().st_mtime)\n",
    "\n",
    "        with open(latest_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation results: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_evaluations():\n",
    "    \"\"\"Load all evaluation results as a list\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        for file_path in EVAL_RESULTS_PATH.glob('*.json'):\n",
    "            # Skip the latest.json symlink/copy\n",
    "            if file_path.name == 'latest.json':\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # Add filename/date information\n",
    "                data['_filename'] = file_path.name\n",
    "                data['_date'] = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "                results.append(data)\n",
    "\n",
    "        # Sort by date\n",
    "        results.sort(key=lambda x: x.get('_date'))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation results: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_user_feedback():\n",
    "    \"\"\"Load user feedback data\"\"\"\n",
    "    try:\n",
    "        if FEEDBACK_PATH.exists():\n",
    "            with open(FEEDBACK_PATH, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"query_feedback\": {}}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading feedback data: {e}\")\n",
    "        return {\"query_feedback\": {}}\n",
    "\n",
    "# Load the data\n",
    "latest_eval = load_latest_evaluation()\n",
    "all_evals = load_all_evaluations()\n",
    "feedback_data = load_user_feedback()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Number of evaluation results found: {len(all_evals)}\")\n",
    "\n",
    "if latest_eval:\n",
    "    print(f\"Latest evaluation has {len(latest_eval.get('metrics', {}))} metrics\")\n",
    "else:\n",
    "    print(\"No evaluation results found. Generate some data first using enhanced_evaluation.py\")\n",
    "\n",
    "print(f\"Number of user feedback entries: {len(feedback_data.get('query_feedback', {}))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616f9b7",
   "metadata": {},
   "source": [
    "## 2. Analyze Operational Metrics\n",
    "\n",
    "Let's visualize the operational metrics from the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_operational_metrics(eval_data):\n",
    "    \"\"\"Extract operational metrics from evaluation data\"\"\"\n",
    "    if not eval_data or 'metrics' not in eval_data:\n",
    "        return {}\n",
    "\n",
    "    metrics = eval_data['metrics']\n",
    "\n",
    "    # Extract operational metrics\n",
    "    operational = {\n",
    "        k: v for k, v in metrics.items()\n",
    "        if any(term in k.lower() for term in ['duration', 'time', 'latency', 'tokens', 'rate'])\n",
    "    }\n",
    "\n",
    "    return operational\n",
    "\n",
    "def plot_operational_metrics(metrics):\n",
    "    \"\"\"Plot operational metrics with appropriate visualizations\"\"\"\n",
    "    if not metrics:\n",
    "        print(\"No operational metrics available.\")\n",
    "        return\n",
    "\n",
    "    # Create subplots with appropriate layout\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"Response Time (seconds)\",\n",
    "            \"Token Usage\",\n",
    "            \"Token Rate (tokens/second)\",\n",
    "            \"Operational Efficiency\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 1. Response Time metrics\n",
    "    time_metrics = {k: v for k, v in metrics.items() if 'time' in k.lower() or 'duration' in k.lower() or 'latency' in k.lower()}\n",
    "    if time_metrics:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(time_metrics.keys()),\n",
    "                y=list(time_metrics.values()),\n",
    "                marker_color='royalblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # 2. Token Usage metrics\n",
    "    token_metrics = {k: v for k, v in metrics.items() if 'token' in k.lower() and 'rate' not in k.lower()}\n",
    "    if token_metrics:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(token_metrics.keys()),\n",
    "                y=list(token_metrics.values()),\n",
    "                marker_color='mediumseagreen'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # 3. Token Rate metrics\n",
    "    rate_metrics = {k: v for k, v in metrics.items() if 'rate' in k.lower()}\n",
    "    if rate_metrics:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(rate_metrics.keys()),\n",
    "                y=list(rate_metrics.values()),\n",
    "                marker_color='darkorange'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    # 4. Efficiency metrics (custom calculated if not present)\n",
    "    efficiency_metrics = {}\n",
    "    if 'completion-tokens' in metrics and 'prompt-tokens' in metrics:\n",
    "        efficiency_metrics['completion-prompt-ratio'] = metrics['completion-tokens'] / max(metrics['prompt-tokens'], 1)\n",
    "    if 'server-run-duration-in-seconds' in metrics and 'completion-tokens' in metrics:\n",
    "        efficiency_metrics['tokens-per-second'] = metrics['completion-tokens'] / max(metrics['server-run-duration-in-seconds'], 0.001)\n",
    "\n",
    "    if efficiency_metrics:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(efficiency_metrics.keys()),\n",
    "                y=list(efficiency_metrics.values()),\n",
    "                marker_color='purple'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        title_text=\"Operational Metrics Analysis\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Extract and plot operational metrics if available\n",
    "if latest_eval:\n",
    "    op_metrics = extract_operational_metrics(latest_eval)\n",
    "    plot_operational_metrics(op_metrics)\n",
    "else:\n",
    "    print(\"No evaluation data available to analyze operational metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be2c95",
   "metadata": {},
   "source": [
    "## 3. Response Quality Analysis\n",
    "\n",
    "Let's analyze the quality metrics of agent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e364c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quality_metrics(eval_data):\n",
    "    \"\"\"Extract quality metrics from evaluation data\"\"\"\n",
    "    if not eval_data or 'metrics' not in eval_data:\n",
    "        return {}\n",
    "\n",
    "    metrics = eval_data['metrics']\n",
    "\n",
    "    # Extract quality metrics\n",
    "    quality = {\n",
    "        k: v for k, v in metrics.items()\n",
    "        if any(term in k.lower() for term in ['quality', 'response_', 'coherence', 'relevance', 'completeness', 'conciseness'])\n",
    "    }\n",
    "\n",
    "    return quality\n",
    "\n",
    "def plot_quality_radar(metrics):\n",
    "    \"\"\"Create a radar chart for quality metrics\"\"\"\n",
    "    if not metrics:\n",
    "        print(\"No quality metrics available.\")\n",
    "        return\n",
    "\n",
    "    # Select metrics to display (exclude overall score to avoid skewing)\n",
    "    display_metrics = {k: v for k, v in metrics.items() if 'overall' not in k.lower()}\n",
    "\n",
    "    # Create radar chart\n",
    "    categories = list(display_metrics.keys())\n",
    "    values = list(display_metrics.values())\n",
    "\n",
    "    # Ensure we have a closed polygon\n",
    "    categories.append(categories[0])\n",
    "    values.append(values[0])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        line=dict(color='rgb(67, 160, 71)'),\n",
    "        fillcolor='rgba(67, 160, 71, 0.2)',\n",
    "        name='Quality Metrics'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )),\n",
    "        title=\"Response Quality Metrics\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Show overall quality score if available\n",
    "    overall_score = metrics.get('overall_quality_score')\n",
    "    if overall_score is not None:\n",
    "        print(f\"Overall Quality Score: {overall_score:.2f}/1.00\")\n",
    "\n",
    "        # Create a gauge chart for overall score\n",
    "        fig = go.Figure(go.Indicator(\n",
    "            mode=\"gauge+number\",\n",
    "            value=overall_score,\n",
    "            domain={'x': [0, 1], 'y': [0, 1]},\n",
    "            title={'text': \"Overall Quality Score\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [0, 1]},\n",
    "                'bar': {'color': \"darkgreen\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 0.33], 'color': \"lightcoral\"},\n",
    "                    {'range': [0.33, 0.67], 'color': \"khaki\"},\n",
    "                    {'range': [0.67, 1], 'color': \"lightgreen\"},\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 0.8\n",
    "                }\n",
    "            }\n",
    "        ))\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "# Extract and plot quality metrics if available\n",
    "if latest_eval:\n",
    "    quality_metrics = extract_quality_metrics(latest_eval)\n",
    "    plot_quality_radar(quality_metrics)\n",
    "else:\n",
    "    print(\"No evaluation data available to analyze quality metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae714e",
   "metadata": {},
   "source": [
    "## 4. Factual Accuracy Analysis\n",
    "\n",
    "Let's analyze the factual accuracy metrics from the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118023a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_accuracy_metrics(eval_data):\n",
    "    \"\"\"Extract accuracy metrics from evaluation data\"\"\"\n",
    "    if not eval_data or 'metrics' not in eval_data:\n",
    "        return {}\n",
    "\n",
    "    metrics = eval_data['metrics']\n",
    "\n",
    "    # Extract accuracy metrics\n",
    "    accuracy = {\n",
    "        k: v for k, v in metrics.items()\n",
    "        if any(term in k.lower() for term in ['accuracy', 'factual', 'correct', 'error', 'mistake'])\n",
    "    }\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def plot_accuracy_metrics(metrics):\n",
    "    \"\"\"Plot accuracy metrics with appropriate visualizations\"\"\"\n",
    "    if not metrics:\n",
    "        print(\"No accuracy metrics available.\")\n",
    "        return\n",
    "\n",
    "    # Sort metrics by value\n",
    "    sorted_metrics = {k: v for k, v in sorted(metrics.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Create horizontal bar chart\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=list(sorted_metrics.values()),\n",
    "        y=list(sorted_metrics.keys()),\n",
    "        orientation='h',\n",
    "        marker=dict(\n",
    "            color='rgba(58, 71, 180, 0.6)',\n",
    "            line=dict(color='rgba(58, 71, 180, 1.0)', width=3)\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Factual Accuracy Metrics\",\n",
    "        xaxis_title=\"Score\",\n",
    "        yaxis_title=\"Metric\",\n",
    "        xaxis=dict(range=[0, 1]),\n",
    "        height=400 + (len(metrics) * 30),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Extract and plot accuracy metrics if available\n",
    "if latest_eval:\n",
    "    accuracy_metrics = extract_accuracy_metrics(latest_eval)\n",
    "    plot_accuracy_metrics(accuracy_metrics)\n",
    "else:\n",
    "    print(\"No evaluation data available to analyze accuracy metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53dc15f",
   "metadata": {},
   "source": [
    "## 5. User Feedback Analysis\n",
    "\n",
    "Let's analyze the user feedback data to understand user satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feedback(feedback_data):\n",
    "    \"\"\"Analyze user feedback data\"\"\"\n",
    "    query_feedback = feedback_data.get('query_feedback', {})\n",
    "\n",
    "    if not query_feedback:\n",
    "        print(\"No user feedback available.\")\n",
    "        return None\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    feedback_list = []\n",
    "\n",
    "    for query, entries in query_feedback.items():\n",
    "        for entry in entries:\n",
    "            feedback_list.append({\n",
    "                'query': query,\n",
    "                'timestamp': entry.get('timestamp', ''),\n",
    "                'rating': entry.get('rating', 0),\n",
    "                'comments': entry.get('comments', ''),\n",
    "                'date': datetime.fromtimestamp(int(entry.get('timestamp', 0)) / 1000) if entry.get('timestamp') else None\n",
    "            })\n",
    "\n",
    "    if not feedback_list:\n",
    "        print(\"No feedback entries found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(feedback_list)\n",
    "    return df\n",
    "\n",
    "def plot_feedback(feedback_df):\n",
    "    \"\"\"Visualize user feedback\"\"\"\n",
    "    if feedback_df is None or feedback_df.empty:\n",
    "        print(\"No feedback data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Create subplots for different visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[\n",
    "            [{'type': 'pie'}, {'type': 'bar'}],\n",
    "            [{'type': 'scatter', 'colspan': 2}, None]\n",
    "        ],\n",
    "        subplot_titles=[\n",
    "            \"Rating Distribution\",\n",
    "            \"Average Rating by Date\",\n",
    "            \"Rating Trend Over Time\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 1. Rating distribution pie chart\n",
    "    rating_counts = feedback_df['rating'].value_counts().sort_index()\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=[f\"{i} Star{'s' if i > 1 else ''}\" for i in rating_counts.index],\n",
    "            values=rating_counts.values,\n",
    "            marker=dict(colors=['#FF4136', '#FF851B', '#FFDC00', '#2ECC40', '#0074D9']),\n",
    "            hole=0.4,\n",
    "            textinfo='label+percent'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Only continue if we have date information\n",
    "    if feedback_df['date'].notna().any():\n",
    "        # 2. Average rating by date\n",
    "        feedback_df['date_only'] = feedback_df['date'].dt.date\n",
    "        daily_ratings = feedback_df.groupby('date_only')['rating'].mean().reset_index()\n",
    "\n",
    "        if not daily_ratings.empty:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=daily_ratings['date_only'],\n",
    "                    y=daily_ratings['rating'],\n",
    "                    marker_color='teal'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "\n",
    "        # 3. Rating trend over time\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=feedback_df.sort_values('date')['date'],\n",
    "                y=feedback_df.sort_values('date')['rating'],\n",
    "                mode='markers+lines',\n",
    "                marker=dict(size=8),\n",
    "                line=dict(width=2, dash='solid')\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"User Feedback Analysis\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"Total feedback entries: {len(feedback_df)}\")\n",
    "    print(f\"Average rating: {feedback_df['rating'].mean():.2f}/5.00\")\n",
    "    print(f\"Rating distribution:\\n{feedback_df['rating'].value_counts().sort_index()}\")\n",
    "\n",
    "# Analyze feedback data\n",
    "feedback_df = analyze_feedback(feedback_data)\n",
    "plot_feedback(feedback_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b28e66",
   "metadata": {},
   "source": [
    "## 6. A/B Testing Comparison\n",
    "\n",
    "Let's compare different agent configurations or versions from multiple evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ab_testing_data(all_evaluations):\n",
    "    \"\"\"Prepare data for A/B testing comparison\"\"\"\n",
    "    if not all_evaluations or len(all_evaluations) < 2:\n",
    "        print(\"Need at least two evaluation results for A/B testing comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Extract key metrics from each evaluation\n",
    "    comparison_data = []\n",
    "\n",
    "    for eval_data in all_evaluations:\n",
    "        metrics = eval_data.get('metrics', {})\n",
    "\n",
    "        # Skip if no metrics available\n",
    "        if not metrics:\n",
    "            continue\n",
    "\n",
    "        # Get key metrics we want to compare\n",
    "        entry = {\n",
    "            'version': eval_data.get('_filename', 'Unknown'),\n",
    "            'date': eval_data.get('_date', datetime.now()).strftime('%Y-%m-%d'),\n",
    "            'response_time': metrics.get('server-run-duration-in-seconds', metrics.get('client-run-duration-in-seconds', 0)),\n",
    "            'tokens': metrics.get('completion-tokens', 0) + metrics.get('prompt-tokens', 0),\n",
    "            'quality': metrics.get('overall_quality_score', 0),\n",
    "            'accuracy': metrics.get('factual_accuracy_score', metrics.get('overall_accuracy', 0)),\n",
    "            'feedback': metrics.get('average_user_rating', 0),\n",
    "        }\n",
    "\n",
    "        comparison_data.append(entry)\n",
    "\n",
    "    if not comparison_data:\n",
    "        print(\"No valid metrics found for comparison.\")\n",
    "        return None\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "def plot_ab_comparison(comparison_df):\n",
    "    \"\"\"Visualize A/B testing comparison\"\"\"\n",
    "    if comparison_df is None or comparison_df.empty:\n",
    "        print(\"No comparison data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Normalize the data for fair comparison\n",
    "    normalized_df = comparison_df.copy()\n",
    "\n",
    "    # Response time: lower is better, so we invert it for visualization\n",
    "    if normalized_df['response_time'].max() > 0:\n",
    "        normalized_df['response_time_normalized'] = 1 - (normalized_df['response_time'] / normalized_df['response_time'].max())\n",
    "    else:\n",
    "        normalized_df['response_time_normalized'] = 0\n",
    "\n",
    "    # Tokens: we normalize but don't invert (neutral metric)\n",
    "    if normalized_df['tokens'].max() > 0:\n",
    "        normalized_df['tokens_normalized'] = normalized_df['tokens'] / normalized_df['tokens'].max()\n",
    "    else:\n",
    "        normalized_df['tokens_normalized'] = 0\n",
    "\n",
    "    # Quality, accuracy, feedback: higher is better\n",
    "    normalized_df['quality_normalized'] = normalized_df['quality']\n",
    "    normalized_df['accuracy_normalized'] = normalized_df['accuracy']\n",
    "    normalized_df['feedback_normalized'] = normalized_df['feedback'] / 5  # Assuming 5-star scale\n",
    "\n",
    "    # Create radar chart for each version\n",
    "    fig = go.Figure()\n",
    "\n",
    "    categories = ['Speed', 'Token Efficiency', 'Quality', 'Accuracy', 'User Satisfaction']\n",
    "\n",
    "    for i, row in normalized_df.iterrows():\n",
    "        values = [\n",
    "            row['response_time_normalized'],\n",
    "            1 - (row['tokens_normalized'] if row['tokens_normalized'] <= 1 else 1),  # Invert tokens for efficiency\n",
    "            row['quality_normalized'],\n",
    "            row['accuracy_normalized'],\n",
    "            row['feedback_normalized']\n",
    "        ]\n",
    "\n",
    "        # Make the polygon closed\n",
    "        categories_closed = categories + [categories[0]]\n",
    "        values_closed = values + [values[0]]\n",
    "\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values_closed,\n",
    "            theta=categories_closed,\n",
    "            fill='toself',\n",
    "            name=f\"{row['version']} ({row['date']})\"\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )),\n",
    "        title=\"A/B Testing Comparison\",\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Create a comparison table\n",
    "    display_df = comparison_df[['version', 'date', 'response_time', 'tokens', 'quality', 'accuracy', 'feedback']]\n",
    "    display_df = display_df.sort_values('date', ascending=False)\n",
    "\n",
    "    display(display_df)\n",
    "\n",
    "# Prepare and plot A/B testing comparison\n",
    "comparison_df = prepare_ab_testing_data(all_evals)\n",
    "plot_ab_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494506df",
   "metadata": {},
   "source": [
    "## 7. Generate Insights and Recommendations\n",
    "\n",
    "Based on the analysis, let's generate insights and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(latest_eval, all_evals, feedback_df):\n",
    "    \"\"\"Generate insights and recommendations based on the evaluation data\"\"\"\n",
    "    insights = []\n",
    "    recommendations = []\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if not latest_eval or 'metrics' not in latest_eval:\n",
    "        insights.append(\"No recent evaluation data available.\")\n",
    "        recommendations.append(\"Run evaluation tests using enhanced_evaluation.py to collect performance data.\")\n",
    "        return insights, recommendations\n",
    "\n",
    "    metrics = latest_eval['metrics']\n",
    "\n",
    "    # 1. Response time insights\n",
    "    response_time = metrics.get('server-run-duration-in-seconds', 0)\n",
    "    if response_time > 5:\n",
    "        insights.append(f\"Response time is relatively high ({response_time:.2f}s).\")\n",
    "        recommendations.append(\"Consider optimizing agent configuration or model selection for faster responses.\")\n",
    "    else:\n",
    "        insights.append(f\"Response time is good ({response_time:.2f}s).\")\n",
    "\n",
    "    # 2. Token usage insights\n",
    "    completion_tokens = metrics.get('completion-tokens', 0)\n",
    "    prompt_tokens = metrics.get('prompt-tokens', 0)\n",
    "    if prompt_tokens > 0 and completion_tokens / prompt_tokens < 0.5:\n",
    "        insights.append(f\"Low completion-to-prompt ratio ({completion_tokens/prompt_tokens:.2f}).\")\n",
    "        recommendations.append(\"Review agent instructions to reduce unnecessary context in prompts.\")\n",
    "\n",
    "    # 3. Quality insights\n",
    "    quality_score = metrics.get('overall_quality_score', 0)\n",
    "    if quality_score < 0.7:\n",
    "        insights.append(f\"Response quality score is below target ({quality_score:.2f}).\")\n",
    "        recommendations.append(\"Improve agent instructions or consider using a more capable model.\")\n",
    "    else:\n",
    "        insights.append(f\"Response quality score is good ({quality_score:.2f}).\")\n",
    "\n",
    "    # 4. Accuracy insights\n",
    "    accuracy_score = metrics.get('factual_accuracy_score', metrics.get('overall_accuracy', 0))\n",
    "    if accuracy_score < 0.8:\n",
    "        insights.append(f\"Factual accuracy is below target ({accuracy_score:.2f}).\")\n",
    "        recommendations.append(\"Enhance knowledge sources or improve grounding in responses.\")\n",
    "    else:\n",
    "        insights.append(f\"Factual accuracy is good ({accuracy_score:.2f}).\")\n",
    "\n",
    "    # 5. User feedback insights\n",
    "    if feedback_df is not None and not feedback_df.empty:\n",
    "        avg_rating = feedback_df['rating'].mean()\n",
    "        if avg_rating < 4:\n",
    "            insights.append(f\"Average user rating is below target ({avg_rating:.2f}/5.00).\")\n",
    "            recommendations.append(\"Review user feedback comments to identify improvement areas.\")\n",
    "        else:\n",
    "            insights.append(f\"User satisfaction is good ({avg_rating:.2f}/5.00).\")\n",
    "    else:\n",
    "        insights.append(\"No user feedback data available.\")\n",
    "        recommendations.append(\"Implement feedback collection in the user interface.\")\n",
    "\n",
    "    # 6. Trend insights\n",
    "    if len(all_evals) >= 2:\n",
    "        # Check for trends in quality or performance\n",
    "        latest_metrics = metrics\n",
    "        previous_metrics = all_evals[-2].get('metrics', {})\n",
    "\n",
    "        if previous_metrics:\n",
    "            prev_quality = previous_metrics.get('overall_quality_score', 0)\n",
    "            curr_quality = latest_metrics.get('overall_quality_score', 0)\n",
    "\n",
    "            if curr_quality > prev_quality:\n",
    "                insights.append(f\"Quality improved since last evaluation (+{curr_quality-prev_quality:.2f}).\")\n",
    "            elif curr_quality < prev_quality:\n",
    "                insights.append(f\"Quality decreased since last evaluation (-{prev_quality-curr_quality:.2f}).\")\n",
    "                recommendations.append(\"Review recent changes to identify potential regressions.\")\n",
    "\n",
    "    # Add general recommendations if we don't have many specific ones\n",
    "    if len(recommendations) < 2:\n",
    "        recommendations.append(\"Continue monitoring agent performance with regular evaluations.\")\n",
    "        recommendations.append(\"Consider implementing A/B testing to compare different agent configurations.\")\n",
    "\n",
    "    return insights, recommendations\n",
    "\n",
    "# Generate insights and recommendations\n",
    "insights, recommendations = generate_insights(latest_eval, all_evals, feedback_df)\n",
    "\n",
    "# Display insights and recommendations\n",
    "print(\"📊 Key Insights:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"  {i}. {insight}\")\n",
    "\n",
    "print(\"\\n📝 Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b312b2",
   "metadata": {},
   "source": [
    "## 8. Export Dashboard as HTML Report\n",
    "\n",
    "Let's export the dashboard as an HTML report that can be shared with stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bad141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_report(latest_eval, all_evals, feedback_df, insights, recommendations):\n",
    "    \"\"\"Create an HTML report with the evaluation results\"\"\"\n",
    "    if not latest_eval:\n",
    "        print(\"No evaluation data available to create report.\")\n",
    "        return\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    # Create a HTML string\n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>AI Agent Evaluation Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .header {{ background-color: #4285f4; color: white; padding: 20px; margin-bottom: 20px; }}\n",
    "            .section {{ margin-bottom: 30px; padding: 20px; border: 1px solid #ddd; border-radius: 5px; }}\n",
    "            .insights {{ background-color: #f8f9fa; }}\n",
    "            .recommendations {{ background-color: #e8f0fe; }}\n",
    "            h1 {{ margin: 0; }}\n",
    "            h2 {{ color: #4285f4; }}\n",
    "            .metric {{ display: inline-block; width: 30%; margin: 10px; padding: 15px; background-color: #f5f5f5; border-radius: 5px; text-align: center; }}\n",
    "            .metric-value {{ font-size: 24px; font-weight: bold; color: #4285f4; }}\n",
    "            .metric-label {{ font-size: 14px; color: #555; }}\n",
    "            table {{ width: 100%; border-collapse: collapse; }}\n",
    "            th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"header\">\n",
    "            <h1>AI Agent Evaluation Report</h1>\n",
    "            <p>Generated on {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}</p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Summary Metrics</h2>\n",
    "    \"\"\"\n",
    "\n",
    "    # Add key metrics\n",
    "    metrics = latest_eval.get('metrics', {})\n",
    "    key_metrics = [\n",
    "        ('Response Time', f\"{metrics.get('server-run-duration-in-seconds', 0):.2f}s\"),\n",
    "        ('Quality Score', f\"{metrics.get('overall_quality_score', 0):.2f}/1.00\"),\n",
    "        ('Accuracy', f\"{metrics.get('factual_accuracy_score', metrics.get('overall_accuracy', 0)):.2f}/1.00\"),\n",
    "        ('Token Usage', f\"{metrics.get('completion-tokens', 0) + metrics.get('prompt-tokens', 0)}\"),\n",
    "    ]\n",
    "\n",
    "    if feedback_df is not None and not feedback_df.empty:\n",
    "        key_metrics.append(('User Rating', f\"{feedback_df['rating'].mean():.2f}/5.00\"))\n",
    "\n",
    "    for label, value in key_metrics:\n",
    "        html += f\"\"\"\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{value}</div>\n",
    "                <div class=\"metric-label\">{label}</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    # Add insights section\n",
    "    html += f\"\"\"\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section insights\">\n",
    "            <h2>Key Insights</h2>\n",
    "            <ul>\n",
    "    \"\"\"\n",
    "\n",
    "    for insight in insights:\n",
    "        html += f\"<li>{insight}</li>\\n\"\n",
    "\n",
    "    html += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section recommendations\">\n",
    "            <h2>Recommendations</h2>\n",
    "            <ul>\n",
    "    \"\"\"\n",
    "\n",
    "    for rec in recommendations:\n",
    "        html += f\"<li>{rec}</li>\\n\"\n",
    "\n",
    "    # Add comparison table if available\n",
    "    if len(all_evals) >= 2:\n",
    "        comparison_df = prepare_ab_testing_data(all_evals)\n",
    "        if comparison_df is not None and not comparison_df.empty:\n",
    "            html += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h2>Evaluation Comparison</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Version</th>\n",
    "                    <th>Date</th>\n",
    "                    <th>Response Time</th>\n",
    "                    <th>Tokens</th>\n",
    "                    <th>Quality</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>User Rating</th>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "\n",
    "            for _, row in comparison_df.sort_values('date', ascending=False).iterrows():\n",
    "                html += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{row['version']}</td>\n",
    "                    <td>{row['date']}</td>\n",
    "                    <td>{row['response_time']:.2f}s</td>\n",
    "                    <td>{row['tokens']}</td>\n",
    "                    <td>{row['quality']:.2f}</td>\n",
    "                    <td>{row['accuracy']:.2f}</td>\n",
    "                    <td>{row['feedback']:.2f}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "\n",
    "            html += \"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "            \"\"\"\n",
    "    else:\n",
    "        html += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    html += \"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>About</h2>\n",
    "            <p>This report was generated by the AI Agent Evaluation Dashboard.</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Save to file\n",
    "    report_path = Path('../evals/evaluation_results/evaluation_report.html')\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "    print(f\"HTML report saved to {report_path}\")\n",
    "    return report_path\n",
    "\n",
    "# Generate HTML report\n",
    "if latest_eval:\n",
    "    report_path = create_html_report(latest_eval, all_evals, feedback_df, insights, recommendations)\n",
    "else:\n",
    "    print(\"No evaluation data available to create report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e70ab4",
   "metadata": {},
   "source": [
    "## 9. Add Sample Data for Development\n",
    "\n",
    "If you don't have real evaluation data yet, you can use this cell to generate sample data for development and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample evaluation and feedback data for development\"\"\"\n",
    "    import random\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    EVAL_RESULTS_PATH = Path('../evals/evaluation_results')\n",
    "    FEEDBACK_PATH = Path('../evals/feedback_data.json')\n",
    "\n",
    "    EVAL_RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Sample queries\n",
    "    sample_queries = [\n",
    "        \"How does this agent work?\",\n",
    "        \"What are the key features of this application?\",\n",
    "        \"Can you explain how the file search works?\",\n",
    "        \"Tell me about agent personalities\",\n",
    "        \"How is monitoring and evaluation implemented?\"\n",
    "    ]\n",
    "\n",
    "    # Generate 3 sample evaluation results\n",
    "    for i in range(3):\n",
    "        # Create base metrics with some randomness\n",
    "        base_quality = 0.7 + (i * 0.05) + random.uniform(-0.05, 0.05)\n",
    "        base_accuracy = 0.75 + (i * 0.05) + random.uniform(-0.05, 0.05)\n",
    "        base_response_time = 3 - (i * 0.3) + random.uniform(-0.2, 0.2)\n",
    "\n",
    "        metrics = {\n",
    "            # Operational metrics\n",
    "            \"server-run-duration-in-seconds\": max(0.5, base_response_time),\n",
    "            \"client-run-duration-in-seconds\": max(0.6, base_response_time + 0.1),\n",
    "            \"completion-tokens\": random.randint(150, 250),\n",
    "            \"prompt-tokens\": random.randint(300, 500),\n",
    "            \"token-generation-rate\": random.uniform(10, 15),\n",
    "\n",
    "            # Quality metrics\n",
    "            \"response_completeness\": min(1.0, base_quality + random.uniform(-0.1, 0.1)),\n",
    "            \"response_relevance\": min(1.0, base_quality + random.uniform(-0.1, 0.1)),\n",
    "            \"response_conciseness\": min(1.0, base_quality + random.uniform(-0.1, 0.1)),\n",
    "            \"overall_quality_score\": min(1.0, base_quality),\n",
    "\n",
    "            # Accuracy metrics\n",
    "            \"factual_accuracy_score\": min(1.0, base_accuracy),\n",
    "            \"citation_accuracy\": min(1.0, base_accuracy + random.uniform(-0.1, 0.1)),\n",
    "            \"hallucination_rate\": max(0, 1 - base_accuracy + random.uniform(-0.1, 0.1)),\n",
    "\n",
    "            # User feedback metrics\n",
    "            \"average_user_rating\": min(5.0, (base_quality + base_accuracy) * 2.5),\n",
    "\n",
    "            # Tool metrics\n",
    "            \"tool_call_accuracy\": min(1.0, 0.85 + (i * 0.03) + random.uniform(-0.05, 0.05)),\n",
    "            \"intent_resolution\": min(1.0, 0.87 + (i * 0.03) + random.uniform(-0.05, 0.05)),\n",
    "            \"task_adherence\": min(1.0, 0.9 + (i * 0.02) + random.uniform(-0.05, 0.05)),\n",
    "\n",
    "            # Safety metrics\n",
    "            \"content_safety_score\": min(1.0, 0.95 + random.uniform(-0.03, 0.03)),\n",
    "            \"attack_resistance\": min(1.0, 0.92 + random.uniform(-0.03, 0.03)),\n",
    "        }\n",
    "\n",
    "        # Create the evaluation result\n",
    "        result = {\n",
    "            \"version\": f\"v1.{i}\",\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"evaluation_name\": f\"sample-evaluation-{i+1}\",\n",
    "            \"metrics\": metrics,\n",
    "            \"config\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"evaluators\": [\"operational\", \"quality\", \"accuracy\", \"feedback\", \"safety\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save to file\n",
    "        timestamp = (datetime.now() - timedelta(days=2-i)).strftime(\"%Y%m%d%H%M%S\")\n",
    "        filename = f\"sample_eval_{timestamp}.json\"\n",
    "\n",
    "        with open(EVAL_RESULTS_PATH / filename, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        # Also save the last one as latest.json\n",
    "        if i == 2:\n",
    "            with open(EVAL_RESULTS_PATH / 'latest.json', 'w') as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "\n",
    "    # Generate sample feedback data\n",
    "    feedback_data = {\"query_feedback\": {}}\n",
    "\n",
    "    for query in sample_queries:\n",
    "        # Generate 2-5 feedback entries for each query\n",
    "        entries = []\n",
    "        for _ in range(random.randint(2, 5)):\n",
    "            # Create timestamps over the past week\n",
    "            days_ago = random.randint(0, 7)\n",
    "            timestamp = int((datetime.now() - timedelta(days=days_ago)).timestamp() * 1000)\n",
    "\n",
    "            entries.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"rating\": random.randint(3, 5),  # Skewed toward positive ratings\n",
    "                \"comments\": random.choice([\n",
    "                    \"Good response, thanks!\",\n",
    "                    \"Very helpful information\",\n",
    "                    \"Could be more detailed\",\n",
    "                    \"Perfect answer\",\n",
    "                    \"\"  # Some entries without comments\n",
    "                ])\n",
    "            })\n",
    "\n",
    "        feedback_data[\"query_feedback\"][query] = entries\n",
    "\n",
    "    # Save feedback data\n",
    "    with open(FEEDBACK_PATH, 'w') as f:\n",
    "        json.dump(feedback_data, f, indent=2)\n",
    "\n",
    "    print(f\"Generated {3} sample evaluation files in {EVAL_RESULTS_PATH}\")\n",
    "    print(f\"Generated sample feedback data with {len(sample_queries)} queries in {FEEDBACK_PATH}\")\n",
    "\n",
    "# Uncomment this line if you want to generate sample data\n",
    "# generate_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208e8d2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has provided comprehensive analysis tools for the AI agent monitoring and evaluation framework. You can use these tools to:\n",
    "\n",
    "1. Monitor agent performance over time\n",
    "2. Compare different agent configurations\n",
    "3. Analyze user feedback and satisfaction\n",
    "4. Generate insights and recommendations for improvement\n",
    "\n",
    "For ongoing monitoring, you can schedule this notebook to run regularly or integrate it into your continuous evaluation pipeline."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
